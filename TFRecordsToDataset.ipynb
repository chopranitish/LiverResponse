{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFRecords to Dataset\n",
    "### This is work to convert .tfrecord files into Datasets, typically used in training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Base_Deeplearning_Code.Data_Generators.TFRecord_to_Dataset_Generator import DataGeneratorClass as DataGenerator\n",
    "from Base_Deeplearning_Code.Data_Generators.Plot_And_Scroll_Images.Plot_Scroll_Images import plot_Image_Scroll_Bar_Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_path = r'K:\\Morfeus\\YHe\\Brian\\deep learning nifti debug\\records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = DataGenerator(record_paths=[record_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How many records do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generator) # The number of unique 2D slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets pull one of the examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = generator.data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get a better understanding of datasets, please review\n",
    "### https://blog.tensorflow.org/2019/02/introducing-tensorflow-datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(data_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_Image_Scroll_Bar_Image(example['image_array'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data for a model\n",
    "For many things, the benefit of datasets is the ability to make on the fly perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Base_Deeplearning_Code.Image_Processors_Module.src.Processors.TFDataSetProcessors as DatasetProcessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processors = [\n",
    "    DatasetProcessors.ExpandDimension(image_keys=('image_array', 'annotation_array'), axis=-1),\n",
    "    DatasetProcessors.ToCategorical(annotation_keys=('annotation_array',), number_of_classes=(2,))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in processors:\n",
    "    example = p.parse(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example['annotation_array'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pause! Before we do anything, we need to make sure we will have an accurate, and reproducible record of what EXACTLY goes into each model. This could be the number of layers in the UNet, the loss function, the learning rate, or ANYTHING\n",
    "### We want to create an excel sheet that will track each parameter to ensure that we can reproduce our model\n",
    "### We will make the FIRST parameter be our 'Model_Index' this will iterate upwards by 1 and be our way of tracking each model made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "model_excel_path = os.path.join('.', 'Model_Parameters.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = ('Layers', 'Filters', 'Convs_Per_Block', 'Minimum_LR', 'Maximum_LR', 'Loss_Function')\n",
    "metrics = ('Loss',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ('Model_Index',) + hyper_parameters + metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(model_excel_path):\n",
    "    data_dict = {}\n",
    "    for key in columns:\n",
    "        data_dict[key] = []\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    df.to_excel(model_excel_path, index=0, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember, you can always add more columns in later, just make sure that they are accounted for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets make a model\n",
    "### Don't forget to pass along the hyper parameters that you will need to fill in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, Filters, blocks=2):\n",
    "    Filters = int(Filters)\n",
    "    for _ in range(blocks):\n",
    "        x = layers.Conv2D(Filters, (3, 3), padding='Same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('selu')(x)\n",
    "    return x\n",
    "def return_model(Layers, Filters, Convs_Per_Block=2):\n",
    "    img_input = x =layers.Input(shape=(512, 512, 1))\n",
    "    concat_values = []\n",
    "    for i in range(Layers - 1):\n",
    "        x = conv_block(x, Filters, blocks=Convs_Per_Block)\n",
    "        concat_values.append(x)\n",
    "        x = layers.MaxPool2D()(x)\n",
    "        Filters *= 2\n",
    "    x = conv_block(x, Filters, blocks=Convs_Per_Block)\n",
    "    for x_i in concat_values[::-1]:\n",
    "        Filters /= 2\n",
    "        x = layers.UpSampling2D()(x)\n",
    "        x = conv_block(x, Filters, blocks=1)\n",
    "        x = layers.Concatenate()([x, x_i])\n",
    "        x = conv_block(x, Filters, blocks=Convs_Per_Block)\n",
    "    x = conv_block(x, Filters, blocks=1)\n",
    "    x = layers.Conv2D(2, (1, 1), padding='Same')(x)\n",
    "    output = layers.Activation('softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs=(img_input,), outputs = (output,))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {'Layers': 2, 'Filters': 16, 'Convs_Per_Block': 2}\n",
    "x = return_model(**model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(plot_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_model(model=x, to_file=os.path.join('.', 'model.png'), show_shapes=True, show_dtype=True,\n",
    "           show_layer_names=False, rankdir='TB', dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
